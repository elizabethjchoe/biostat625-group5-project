---
title: "Impact of Socioeconomic Factors on Heart Disease Incidence in U.S."
author: "Elizabeth Choe, Yaodong Xin, Yijun Guo"
output:
  bookdown::pdf_document2:
    fig_crop: no
    keep_tex: yes
    latex_engine: lualatex
    number_sections: yes
    toc: no
fontsize: 12pt
classoption: twocolumn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\thispagestyle{empty}
\section{Introduction}
\vspace{-0.2cm}
Heart disease is the leading cause of death in the U.S. Main risk factors of heart diseases among Americans including high blood pressure, high cholesterol, smoking, alcohol, and inactivity have been rigorously investigated. In recent years, aside from those physical factors, socioeconomic status(SES) has become a hot topic to determine the risk stratification of several chronic diseases. 

SES is typically represented by income level, employment status, education level, and environmental factors. The discrepancy of SES not only leads to different risk levels of diseases adjusted for other factors but also influences the effect of interventions. For example, the efficacy of behavioral counseling on smoking cessation is more limited among low SES groups than high SES groups, and taking SES markers into account could enhance the precision of risk prediction systems[1]. Though it was shown that heart disease incidence and mortality are higher among middle and low SES groups, the impact of SES on the effects of different risk factors has not been rigorously demonstrated. The dynamic SES under social events (e.g.: COVID-19, economic downturn) also calls for renewal of research findings.

Therefore, in our project, we would like to investigate the influence of SES using 2021’s research data and provide the latest evidence of SES’s function on heart disease risk prediction, which might implicit possible prophylactic steps in different SES groups in America. The computing and visualization codes are in \href{https://github.com/elizabethjchoe/biostat625-group5-project}{625 project} 


\section{Methods}
\vspace{-0.4cm}
\subsection{Dataset}
\vspace{-0.2cm}
The dataset used in our project originates from the Behavioral Risk Factor Surveillance System (BRFSS) of CDC, which includes data of telephone health status surveys across America. The 2021 dataset consists of over 400,000 observations and 279 variables. 22 variables related to heart disease were extracted as subset for analysis in our project: 1) Outcome: the personal reported incidence of heart disease (CHD) or myocardial infarction (MI) was taken as the outcome; 2) SES factors: income level,education level, employment status, and environmental factors about insurance, healthcare provider, and frequency of body check; 3) Confounders: demographic information(age, sex, race), disease history(diabetes, kidney disease, skin disease), life habits(alcohol intake, smoking, exercise), general health indicators(physical/mental health score, BMI). The above variables are renamed for analysis convenience, See original BRFSS variable codes and new codes in \href{https://github.com/elizabethjchoe/biostat625-group5-project}{coding table} and \textbf{Table 1}.

\vspace{-0.4cm}
\subsection{Statistical analyses}
\vspace{-0.2cm}
Statistical analyses were conducted by RStudio, and result visualization were conducted by $ggplot2$, $gtsummary$ and $dplyr$ packages. To begin analysis, since the data was derived from social telephone survey, transformations of variable categories were firstly conducted. Values represent "Refuse","Don’t know/Not Sure", "Not asked or Missing" were treated as missing values. Unordered categorical variables were treated as dummy variables during modeling. 

After transformation, data visualizations were created to show associations between outcome and other variables by conducting logistic regression. By investigating the distribution of values in each categories, we also found that there exist unbalanced data in our data set, especially for outcomes (See detailed analysis in  \href{liz/Transformation and descriptive.Rmd}{transformation and descriptive}). Proportions of missing values for each variable are also displayed in \textbf{Table 1}, and \textbf{Figure 1} further demonstrated the nonrandom missing pattern for variables.

![](C:/Users/guoyijun/Desktop/course/625/project1/Variable Category-1.jpg){width=50%} 
![](C:/Users/guoyijun/Desktop/course/625/project1/Figure 1 Missing Values-1.jpg){width=40%,helght=40%}

\vspace{-0.4cm}
\subsection{Data processing}
\vspace{-0.2cm}
\subsubsection{Missing Values Imputation}
\vspace{-0.4cm}
The nonrandom missing pattern and missing abundance called for imputation.KNN method was applied to conduct imputation.
 
\subsubsection{Unbalanced Labels}
\vspace{-0.4cm}
According the statistical analyses, there are 91.8\% (398,735) normal observations and 8.2\% (35,323) observations with heart disease in our dataset. Such unbalanced label pattern would introduce bias when classifying. To be specific, the classifier may pay more attention on the majority label rather than treat two labels equally. Several methods could be utilized to tackle this problem, e.g. assigning different weights to each class, sampling techniques, and changing threshold. Among these methods, a sampling method called Synthetic Minority Oversampling Technique (SMOTE) is broadly used for its simplicity and efficiency without loss of much information. SMOTE is a typical data augmentation technique combining the ideas of both oversampling and undersampling, automatically generates new samples from current dataset. It firstly chose a sample randomly from the minority class, found its $k$ nearest neighbors and create a new sample according to its neighbors' features. It can also perform undersampling among majority dataset to further sustain the balance.

\vspace{-0.4cm}
\subsection{Models}
\vspace{-0.2cm}
\subsubsection{Logistic Regression}
\vspace{-0.4cm}
Logistic regression (LR) is a type of generalized linear model, which means that it is a linear model that is used to predict a categorical response. It is called "logistic" because it uses the logistic function to model the probability of an event occurring (\textbf{Figure 2}). The logistic function is a sigmoid curve that maps any input value to a value between 0 and 1. This allows the model to predict the probability that an example belongs to a certain class. LR is a widely used and effective method for binary classification. It is a simple and interpretable model that can be trained efficiently. However, it is sensitive to imbalanced classes and can be prone to overfitting if the number of features is very large relative to the number of examples.

![](C:/Users/guoyijun/Desktop/course/625/project1/logistic function.PNG){width=50%} 
\vspace{-0.4cm}
\subsubsection{XGBoost}
\vspace{-0.4cm}
XGBoost (eXtreme Gradient Boosting)[2] is a powerful and widely-used gradient boosting algorithm in machine learning. Gradient boosting is a machine learning technique that creates a predictive model by adding a sequence of weak learners to an ensemble model. The base models are trained on the residual errors of the previous one, with the goal of minimizing the overall prediction error. XGBoost is an implementation of gradient boosting that is optimized for faster training and higher performance, using techniques such as multi-core CPU parallelization for big data problems. It has also provided a variety of hyperparameters that can be tuned to customize the model's learning process and improve its performance. XGBoost has been proven in reality to be effective in a variety of applications, including classification and regression tasks.

\vspace{-0.4cm}
\subsubsection{XGBoost + SparseLR}
\vspace{-0.4cm}
Typically, the LR model can only handle linear association between the outcome and predictors. It is always time consuming and sometimes subjective to extract features from the original dataset manually. Many researchers have explored various ways to perform an efficient feature engineering. GBDT+LR[3] was a combined machine learning model proposed to process and extract the information in the dataset automatically. To be specific, we firstly train a gradient boosting tree model. For each tree in the ensemble model, every data row is going to fall on one of the leaf nodes, forming a new feature vector. Suppose there are $k$ trees in the model and each tree has $l$ leaf nodes on average. The original $n\times m$ dataset will then be transformed into a $n\times kl$ dataset, with each row as an one-hot vector. Since the new dataset only contains 0 and 1 and most them are 0, we could apply sparse matrix algorithm to accelerate the latter Logistic Regression module. However, there is also some argument about whether the combination of GBDT and LR really outperforms a sole GBDT model. Therefore, we are also doing experiments \footnote{For better implementation in R, we choose XGBoost instead of a vanilla GBDT model.}to compare the result of different models.
![](C:/Users/guoyijun/Desktop/course/625/project1/gbdtlr_v2.PNG){width=40%,height=40%} 

\vspace{-0.4cm}
\subsection{Evaluation}
\vspace{-0.2cm}
\subsubsection{Data Split}
\vspace{-0.4cm}
We firstly divide our dataset into the train set (80\%) and the test set (20\%). For all of our models mentioned above, we would use the train dataset to train the model and evaluate it on both sets but mainly focus on the result of the test set.

\vspace{-0.4cm}
\subsubsection{Hyperparameters Searching}
\vspace{-0.4cm}
Note that for SMOTE sampling method and XGBoost model, there are a set of hyperparameters to determine. For example, the number of neighbors chosen $k$ in SMOTE, the number of trees and the nodes of each tree in XGBoost, etc. A feasible solution is to perform a 5-fold cross-validation on train dataset. To be specific, we firstly decide a rough range of the hyperparameters and divide the train dataset into 5 equal subsets. And then we use the four of them to try possible hyperparameters and select the best hyperparameters based on the performance of the rest one subset. The optimal hyperparameters used in our model are listed in table \textbf{Table 2}.

\begin{table}[h]
\caption*{Table2 Optimal hyperparameters used in our model}
\centering
\begin{tabular}{c c c}
\hline
Model & Hyperparameter & Value \\
\hline
SMOTE & \# neighbors & \\
SMOTE & perc. & \\
XGBoost & learning rate & 0.8 \\
XGBoost & \# trees & 100 \\
XGBoost & max depth & 3 \\
\hline
\end{tabular}
\label{hyp}
\end{table}


\vspace{-0.4cm}
\section{Results}
\vspace{-0.2cm}

\vspace{-0.4cm}
\section{Conclusion and Discussion}
\vspace{-0.2cm}



\end{document}


\section*{References}
[1] Schultz WM, Kelli HM, Lisko JC, Varghese T, Shen J, Sandesara P, Quyyumi AA, Taylor HA, Gulati M, Harold JG, Mieres JH, Ferdinand KC, Mensah GA, Sperling LS. Socioeconomic Status and Cardiovascular Outcomes: Challenges and Interventions. Circulation. 2018 May 15;137(20):2166-2178. doi: 10.1161/CIRCULATIONAHA.117.029652. PMID: 29760227; PMCID: PMC5958918.

[2] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16). Association for Computing Machinery, New York, NY, USA, 785–794. https://doi.org/10.1145/2939672.2939785

[3] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, and Joaquin Quiñonero Candela. 2014. Practical Lessons from Predicting Clicks on Ads at Facebook. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising (ADKDD'14). Association for Computing Machinery, New York, NY, USA, 1–9. https://doi.org/10.1145/2648584.2648589

